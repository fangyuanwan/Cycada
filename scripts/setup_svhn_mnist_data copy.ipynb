{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from cycada.data.data_loader import get_dataset\n",
    "import PIL.Image as Image\n",
    "from os.path import join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "mnist_dataset = datasets.MNIST('../x/jhoffman/mnist/', train=True, transform=None, \n",
    "                               target_transform=None, download=True)\n",
    "svhn_dataset = datasets.SVHN('../x/jhoffman/svhn/', split='train', transform=None, \n",
    "                             target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def process_dataset(gt_folder, img_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    images_output_folder = os.path.join(output_folder, 'images')\n",
    "    if not os.path.exists(images_output_folder):\n",
    "        os.makedirs(images_output_folder)\n",
    "\n",
    "    labels_file = os.path.join(output_folder, 'labels.txt')\n",
    "    with open(labels_file, 'w') as labels_f:\n",
    "        img_index = 0\n",
    "\n",
    "        for gt_file in os.listdir(gt_folder):\n",
    "            gt_path = os.path.join(gt_folder, gt_file)\n",
    "            img_name = gt_file.replace('gt_', '').replace('.txt', '.jpg')\n",
    "            img_path = os.path.join(img_folder, img_name)\n",
    "\n",
    "            if os.path.exists(img_path):\n",
    "                img = Image.open(img_path)\n",
    "                with open(gt_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split(',')\n",
    "                        if len(parts) == 9:\n",
    "                            coordinates = list(map(int, parts[:8]))\n",
    "                            label = parts[8]\n",
    "\n",
    "                            # Define bounding box\n",
    "                            left = min(coordinates[0::2])\n",
    "                            top = min(coordinates[1::2])\n",
    "                            right = max(coordinates[0::2])\n",
    "                            bottom = max(coordinates[1::2])\n",
    "\n",
    "                            # Crop and save image\n",
    "                            cropped_img = img.crop((left, top, right, bottom))\n",
    "                            cropped_img_path = os.path.join(images_output_folder, f'{img_index}.png')\n",
    "                            cropped_img.save(cropped_img_path)\n",
    "\n",
    "                            # Write label\n",
    "                            labels_f.write(f'{img_index} {label}\\n')\n",
    "                            img_index += 1\n",
    "\n",
    "# Process each dataset\n",
    "process_dataset('../2013/Challenge2_Training_Task1_GT', '../2013/Challenge2_Training_Task12_Images', '../FY/Train/TrainA')\n",
    "process_dataset('../2013/Challenge2_Test_Task12_Images', '../2013/Challenge2_Test_Task12_Images 2', '../FY/Test/TestA')\n",
    "process_dataset('../2015/ch4_training_localization_transcription_gt', '../2015/ch4_training_images', '../FY/Train/TrainB')\n",
    "process_dataset('../2015/Challenge4_Test_Task1_GT', '../2015/ch4_test_images', '../FY/Test/TestB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def process_dataset(gt_folder, img_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    images_output_folder = os.path.join(output_folder, 'images')\n",
    "    if not os.path.exists(images_output_folder):\n",
    "        os.makedirs(images_output_folder)\n",
    "\n",
    "    labels_file = os.path.join(output_folder, 'labels.txt')\n",
    "    with open(labels_file, 'w') as labels_f:\n",
    "        img_index = 0\n",
    "\n",
    "        for gt_file in os.listdir(gt_folder):\n",
    "            gt_path = os.path.join(gt_folder, gt_file)\n",
    "            img_name = gt_file.replace('gt_', '').replace('.txt', '.jpg')\n",
    "            img_path = os.path.join(img_folder, img_name)\n",
    "\n",
    "            if os.path.exists(img_path):\n",
    "                img = Image.open(img_path)\n",
    "                try:\n",
    "                    with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "                        lines = f.readlines()\n",
    "                except UnicodeDecodeError:\n",
    "                    with open(gt_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                        lines = f.readlines()\n",
    "\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split(',')\n",
    "                    if len(parts) == 9:\n",
    "                        coordinates = list(map(int, parts[:8]))\n",
    "                        label = parts[8]\n",
    "\n",
    "                        # Define bounding box\n",
    "                        left = min(coordinates[0::2])\n",
    "                        top = min(coordinates[1::2])\n",
    "                        right = max(coordinates[0::2])\n",
    "                        bottom = max(coordinates[1::2])\n",
    "\n",
    "                        # Crop and save image\n",
    "                        cropped_img = img.crop((left, top, right, bottom))\n",
    "                        cropped_img_path = os.path.join(images_output_folder, f'{img_index}.png')\n",
    "                        cropped_img.save(cropped_img_path)\n",
    "\n",
    "                        # Write label\n",
    "                        labels_f.write(f'{img_index} {label}\\n')\n",
    "                        img_index += 1\n",
    "\n",
    "# Process each dataset\n",
    "process_dataset('../2013/Challenge2_Training_Task1_GT', '../2013/Challenge2_Training_Task12_Images', '../FY/Train/TrainA')\n",
    "process_dataset('../2013/Challenge2_Test_Task12_Images', '../2013/Challenge2_Test_Task12_Images 2', '../FY/Test/TestA')\n",
    "process_dataset('../2015/ch4_training_localization_transcription_gt', '../2015/ch4_training_images', '../FY/Train/TrainB')\n",
    "process_dataset('../2015/Challenge4_Test_Task1_GT', '../2015/ch4_test_images', '../FY/Test/TestB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_vocabulary(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        vocabulary = [line.strip() for line in file]\n",
    "\n",
    "    word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "    word_to_index[\"###\"] = len(word_to_index)  # Handle special cases\n",
    "\n",
    "    return word_to_index\n",
    "\n",
    "def word_to_label(word, word_to_index):\n",
    "    return word_to_index.get(word, 1)  # Returns the index for \"###\" if the word is not found\n",
    "\n",
    "def process_dataset(gt_folder, img_folder, output_folder, word_to_index, numeric_naming=False, diagonal_points=False):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    images_output_folder = os.path.join(output_folder, 'images')\n",
    "    if not os.path.exists(images_output_folder):\n",
    "        os.makedirs(images_output_folder)\n",
    "\n",
    "    labels_file = os.path.join(output_folder, 'labels.txt')\n",
    "    \n",
    "    with open(labels_file, 'w') as labels_f:\n",
    "        print(labels_f)\n",
    "        img_index = 0\n",
    "\n",
    "        for gt_file in os.listdir(gt_folder):\n",
    "            gt_path = os.path.join(gt_folder, gt_file)\n",
    "            print(gt_path)\n",
    "\n",
    "            if numeric_naming:\n",
    "                # Extract the numeric part for numeric naming convention\n",
    "                numeric_part = gt_file.split('_')[1].split('.')[0]  # Extracts the number from '100.txt'\n",
    "                img_name = f'{numeric_part}.jpg'  # Constructs the corresponding image filename\n",
    "            else:\n",
    "                # Extract the numeric part for 'img_X.jpg' naming convention\n",
    "                numeric_part = gt_file.split('_')[2].split('.')[0]\n",
    "                img_name = f'img_{numeric_part}.jpg'\n",
    "\n",
    "            img_path = os.path.join(img_folder, img_name)\n",
    "            print(img_path)\n",
    "\n",
    "            if os.path.exists(img_path):\n",
    "                img = Image.open(img_path)\n",
    "                try:\n",
    "                    with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "                        lines = f.readlines()\n",
    "                except UnicodeDecodeError:\n",
    "                    with open(gt_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                        lines = f.readlines()\n",
    "\n",
    "                for line in lines:\n",
    "\n",
    "                    if ',' in line:\n",
    "                        parts = line.strip().split(',')\n",
    "                        print (parts)\n",
    "                    else:\n",
    "                        parts = line.strip().split(' ')\n",
    "                        print (parts)\n",
    "                   \n",
    "                    if ((len(parts) == 9) or (len(parts) == 5)):\n",
    "                       \n",
    "                        if (len(parts) == 5):\n",
    "                            try:\n",
    "                                coordinates = list(map(int, parts[:4]))\n",
    "                                label_word = parts[4].upper()\n",
    "                                print(label_word)\n",
    "                                label_index = word_to_label(label_word, word_to_index)  # Convert word to label index \n",
    "                                print(label_index)\n",
    "\n",
    "                                if (label_index == 87623):\n",
    "                                    label_index=0\n",
    "                                else:\n",
    "                                    label_index=1\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                \n",
    "                            # If coordinates are diagonal points (x1, y1, x3, y3)\n",
    "                            x1, y1, x3, y3 = coordinates[0], coordinates[1], coordinates[2], coordinates[3]\n",
    "                            left, top = min(x1, x3), min(y1, y3)\n",
    "                            right, bottom = max(x1, x3), max(y1, y3)\n",
    "                        else:\n",
    "                            try:\n",
    "                                coordinates = list(map(int, parts[:8]))\n",
    "                                label_word = parts[8]\n",
    "                                print (label_word)\n",
    "                                \n",
    "                                label_index = word_to_label(label_word, word_to_index)  # Convert word to label index \n",
    "                                if (label_index == 87623):\n",
    "                                    label_index=0\n",
    "                                else:\n",
    "                                    label_index=1\n",
    "\n",
    "\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                            # Regular bounding box coordinates\n",
    "                            left, top, right, bottom = min(coordinates[0::2]), min(coordinates[1::2]), max(coordinates[0::2]), max(coordinates[1::2])\n",
    "\n",
    "                        # Define bounding box and crop image\n",
    "                        cropped_img = img.crop((left, top, right, bottom))\n",
    "                        cropped_img_path = os.path.join(images_output_folder, f'{img_index}.png')\n",
    "                        cropped_img.save(cropped_img_path)\n",
    "                        print(cropped_img_path)\n",
    "\n",
    "                        # Write label index\n",
    "                        labels_f.write(f'{img_index} {label_index}\\n')\n",
    "                        img_index += 1\n",
    "# Load vocabulary\n",
    "vocabulary_file = '../GenericVocabulary.txt'  # Replace with the path to your vocabulary file\n",
    "word_to_index = load_vocabulary(vocabulary_file)\n",
    "\n",
    "# Process each dataset\n",
    "process_dataset('../2013/Challenge2_Training_Task1_GT', '../2013/Challenge2_Training_Task12_Images', '../FY/train/trainA', word_to_index, numeric_naming=True, diagonal_points=True)\n",
    "process_dataset('../2013/Challenge2_Test_Task1_GT', '../2013/Challenge2_Test_Task12_Images', '../FY/test/testA',word_to_index, numeric_naming=False, diagonal_points=True)\n",
    "process_dataset('../2015/ch4_training_localization_transcription_gt', '../2015/ch4_training_images', '../FY/train/trainB',word_to_index, numeric_naming=False, diagonal_points=False)\n",
    "process_dataset('../2015/Challenge4_Test_Task1_GT', '../2015/ch4_test_images', '../FY/test/testB',word_to_index, numeric_naming=False,diagonal_points=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# Load vocabulary and process datasets, using the numeric_naming parameter as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '../x/jhoffman/cyclegan_data/svhn2mnist'\n",
    "# SVHN->MNIST: convert to 32x32x3 images\n",
    "\n",
    "os.makedirs(join(outdir, 'trainB/images'), exist_ok=True)\n",
    "with open(join(outdir, 'trainB/labels.txt'), 'w') as label_file:\n",
    "    for i in range(len(mnist_dataset)):\n",
    "        img = Image.fromarray(mnist_dataset.train_data[i].numpy())\n",
    "        img = img.resize([32,32])\n",
    "        img = img.convert('RGB')\n",
    "        img.save(join(outdir, 'trainB/images', '{:d}.png'.format(i)))\n",
    "        label_file.write('{:d} {:d}\\n'.format(i, mnist_dataset.train_labels[i]))\n",
    "     \n",
    "\n",
    "os.makedirs(join(outdir, 'trainA/images'), exist_ok=True)\n",
    "svhn_labels = svhn_dataset.labels.flatten()\n",
    "with open(join(outdir, 'trainA/labels.txt'), 'w') as label_file:\n",
    "    for i in range(len(svhn_dataset)):\n",
    "        img = Image.fromarray(svhn_dataset.data[i].transpose(1,2,0))\n",
    "        img.save(join(outdir, 'trainA/images', '{:d}.png'.format(i)))\n",
    "        label_file.write('{:d} {:d}\\n'.format(i, svhn_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def process_dataset(data_folder, output_folder, target_size=(64, 64), convert_to_rgb=False):\n",
    "    images_folder = os.path.join(data_folder, 'images')\n",
    "    labels_file = os.path.join(data_folder, 'labels.txt')\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_folder, 'images'), exist_ok=True)\n",
    "\n",
    "    with open(labels_file, 'r') as labels_f, open(os.path.join(output_folder, 'labels.txt'), 'w') as out_labels_f:\n",
    "        for line in labels_f:\n",
    "            idx, label = line.strip().split()\n",
    "            img_path = os.path.join(images_folder, f'{idx}.png')\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # Resize every image to the target size\n",
    "            img = img.resize(target_size)\n",
    "\n",
    "            if convert_to_rgb:\n",
    "                img = img.convert('RGB')\n",
    "\n",
    "            img.save(os.path.join(output_folder, 'images', f'{idx}.png'))\n",
    "            out_labels_f.write(f'{idx} {label}\\n')\n",
    "\n",
    "# Example usage\n",
    "base_dir = '../FY/train'\n",
    "process_dataset(os.path.join(base_dir, 'trainA'), '../FY/cyclegan_data/ICDAR2013_2015/trainA', target_size=(128, 128), convert_to_rgb=True)\n",
    "process_dataset(os.path.join(base_dir, 'trainB'), '../FY/cyclegan_data/ICDAR2013_2015/trainB', target_size=(128, 128), convert_to_rgb=True)\n",
    "\n",
    "# Example usage\n",
    "base_dir = '../FY/test'\n",
    "process_dataset(os.path.join(base_dir, 'testA'), '../FY/cyclegan_data/ICDAR2013_2015/testA', target_size=(128, 128), convert_to_rgb=True)\n",
    "process_dataset(os.path.join(base_dir, 'testB'), '../FY/cyclegan_data/ICDAR2013_2015/testB', target_size=(128, 128), convert_to_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder copied from ../FY/cyclegan_data/ICDAR2013_2015/trainA to ../FY/icdar2013/train\n",
      "Folder copied from ../FY/cyclegan_data/ICDAR2013_2015/trainB to ../FY/icdar2015/train\n",
      "Folder copied from ../FY/cyclegan_data/ICDAR2013_2015/testA to ../FY/icdar2013/test\n",
      "Folder copied from ../FY/cyclegan_data/ICDAR2013_2015/testB to ../FY/icdar2015/test\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "source_folder = '../FY/cyclegan_data/ICDAR2013_2015/trainA'\n",
    "destination_folder = '../FY/icdar2013/train'\n",
    "\n",
    "# Copy the folder\n",
    "shutil.copytree(source_folder, destination_folder)\n",
    "\n",
    "print(f\"Folder copied from {source_folder} to {destination_folder}\")\n",
    "\n",
    "source_folder = '../FY/cyclegan_data/ICDAR2013_2015/trainB'\n",
    "destination_folder = '../FY/icdar2015/train'\n",
    "\n",
    "# Copy the folder\n",
    "shutil.copytree(source_folder, destination_folder)\n",
    "\n",
    "print(f\"Folder copied from {source_folder} to {destination_folder}\")\n",
    "\n",
    "source_folder = '../FY/cyclegan_data/ICDAR2013_2015/testA'\n",
    "destination_folder = '../FY/icdar2013/test'\n",
    "\n",
    "# Copy the folder\n",
    "shutil.copytree(source_folder, destination_folder)\n",
    "\n",
    "print(f\"Folder copied from {source_folder} to {destination_folder}\")\n",
    "\n",
    "source_folder = '../FY/cyclegan_data/ICDAR2013_2015/testB'\n",
    "destination_folder = '../FY/icdar2015/test'\n",
    "\n",
    "# Copy the folder\n",
    "shutil.copytree(source_folder, destination_folder)\n",
    "\n",
    "print(f\"Folder copied from {source_folder} to {destination_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=1 \n",
    "# !python train.py --name experiment_name \\ \n",
    "# --dataroot path_to_gta2cityscape --resize_or_crop=crop --loadSize=360 --fineSize=360 --identity 1.0 \\\n",
    "# --which_model_netD n_layers --n_layers_D 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable\n",
    "\n",
    "!python ../cyclegan/pytorch-CycleGAN-and-pix2pix/train.py --name test_svhn2mnist \\\n",
    "--dataroot ../x/jhoffman/cyclegan_data/svhn2mnist/ --resize_or_crop=None \\\n",
    "--loadSize=32 --fineSize=32 --which_model_netD n_layers --n_layers_D 3 \\\n",
    "--no_flip --model cycle_gan --lambda_A 1 --lambda_B 1 --lambda_identity 1.0 --gpu_ids 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/Users/pomvrp/Documents/NTU AI Courses/AI6121 computer vision/18 Nov Project/cycada_release/cycada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd '/Users/pomvrp/Documents/NTU AI Courses/AI6121 computer vision/18 Nov Project/cycada_release'\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# A domain adaptive semantic segmentation model that is trained with the translated source data and evaluated over the target data.\n",
    "! python cycada/scripts/train_adda.py --name ICDAR2013to2015-adaptive \\\n",
    "--dataroot ../../FY/cyclegan_data/ICDAR2013_2015/ --resize_or_crop=None \\\n",
    "--loadSize=32 --fineSize=32 --which_model_netD n_layers --n_layers_D 3 \\\n",
    "--no_flip --model LeNet --lambda_A 1 --lambda_B 1 --lambda_identity 1.0 --gpu_ids -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the path to your .sh file\n",
    "script_path = \"scripts/train_fcn_adda.sh\"\n",
    "\n",
    "# Run the shell script\n",
    "subprocess.run([\"bash\", script_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = join(outdir, 'trainB')\n",
    "for i in range(10):\n",
    "    img = Image.open(join(dirname, '{:d}.png'.format(i)))\n",
    "\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.grid('off')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = join(outdir, 'trainA')\n",
    "for i in range(10):\n",
    "    img = Image.open(join(dirname, '{:d}.png'.format(i)))\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.grid('off')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = svhn_dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count,bins = np.histogram(y.squeeze(), bins=10)\n",
    "plt.bar(range(10), count); plt.title('P(Y) for SVHN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_num = min(count)\n",
    "ind = np.zeros((10,min_num), dtype=int)\n",
    "for i in np.unique(y):\n",
    "    binary_ind = np.where(y.squeeze() == i)[0]\n",
    "    np.random.shuffle(binary_ind)\n",
    "    \n",
    "    ind[i-1,:] = binary_ind[:min_num]\n",
    "\n",
    "ind = ind.flatten()\n",
    "np.random.shuffle(ind)\n",
    "y_new = y[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_new,_ = np.histogram(y_new, bins=10)\n",
    "plt.bar(range(10), count_new); plt.title('P(Y) SVHN balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/x/jhoffman/cyclegan_data/svhn2mnist/trainA/labels.txt', 'r') as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "parse = np.array([(int(x.split(' ')[0]), int(x.split(' ')[1])) for x in data])\n",
    "d = dict(parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Print original sys.path\n",
    "print(\"Original sys.path:\", sys.path)\n",
    "\n",
    "# Define the path you want to remove\n",
    "path_to_remove = '..'\n",
    "\n",
    "# Remove the path if it exists in sys.path\n",
    "sys.path = [p for p in sys.path if p != path_to_remove]\n",
    "\n",
    "# Print modified sys.path\n",
    "print(\"Modified sys.path:\", sys.path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
